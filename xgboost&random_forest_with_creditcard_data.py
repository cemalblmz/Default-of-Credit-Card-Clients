# -*- coding: utf-8 -*-
"""XGBoost&Random Forest with Creditcard Data.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1zaTn6NT8HDq-CZS9SJA0xHDIXEcemZ8j

Data is from the following https://archive.ics.uci.edu/ml/datasets/default+of+credit+card+clients

metric to maximize is f1_score
"""

import numpy as np
import pandas as pd
from matplotlib import pyplot as plt
import sklearn.preprocessing as pre
import sklearn.linear_model as lin
import sklearn.model_selection as mod
import sklearn.metrics as met
import sklearn.pipeline as pip
import sklearn.tree as tree
import sklearn.ensemble as ens
import seaborn as sns
from sklearn import cluster as clu
import sklearn.compose as cmp
import xgboost as xgb
from xgboost.sklearn import XGBClassifier
from imblearn import over_sampling as ove
from imblearn import under_sampling as und
from imblearn import combine as com
from imblearn import pipeline as imbPipe
import matplotlib.cbook
from sklearn.model_selection import learning_curve

from google.colab import drive
drive.mount('/content/drive')

#df = pd.read_excel("default of credit card clients.xls",header=1)

df = pd.read_excel("/content/drive/MyDrive/default of credit card clients.xls",header=1)
df.head(2)

pd.concat([df.describe(include="all").T,(round(100*(df.isnull().sum()/len(df.index)), 2)).rename('missing_ratio')], axis=1)

positive_ratio    = (str(round(df[df["default payment next month"] == 1]["default payment next month"].count()/df["default payment next month"].count(),2)*100))
negative_ratio = (str(round(df[df["default payment next month"] == 0]["default payment next month"].count()/df["default payment next month"].count(),2)*100))
sns.set_style("white")
sns.countplot(x='default payment next month', data=df, palette="Set1")
plt.title("positive_ratio: "+ positive_ratio + " - negative_ratio: " + negative_ratio )
plt.show()

sns.heatmap(df.corr(), cmap='seismic');

"""BILL AMT and PAY features have high correlation. I will keep them in the first place. However, I may need to remove them.

Hence, in case of an info loss, I will calculate percentage change for the related columns. For example Pay 0 and pay 2 represent the same info for different months. Percentage change can be a good way to keep the information.

"""

df_Pay = df[["PAY_0","PAY_2","PAY_3","PAY_4","PAY_5","PAY_6"]].pct_change(axis='columns').replace([np.nan,np.inf, -np.inf], 0)
df_Pay = df_Pay[["PAY_2","PAY_3","PAY_4","PAY_5","PAY_6"]].rename(columns={"PAY_2":"PAY_0_2", "PAY_3":"PAY_2_3", "PAY_4":"PAY_3_4", "PAY_5":"PAY_4_5","PAY_6":"PAY_5_6"})
df = pd.concat((df,df_Pay),axis=1)
df.head()

df_bill = df[["BILL_AMT1","BILL_AMT2","BILL_AMT3","BILL_AMT4","BILL_AMT5","BILL_AMT6"]].pct_change(axis='columns').replace([np.nan,np.inf, -np.inf], 0)
df_bill = df_bill[["BILL_AMT2","BILL_AMT3","BILL_AMT4","BILL_AMT5","BILL_AMT6"]].rename(columns={"BILL_AMT2":"BILL_AMT1_2", "BILL_AMT3":"BILL_AMT_2_3", "BILL_AMT4":"BILL_AMT_3_4", "BILL_AMT5":"BILL_AMT4_5","BILL_AMT6":"BILL_AMT_5_6"})
df = pd.concat((df,df_bill),axis=1)
df.head()

df_pamt = df[["PAY_AMT1","PAY_AMT2","PAY_AMT3","PAY_AMT4","PAY_AMT5","PAY_AMT6"]].pct_change(axis='columns').replace([np.nan,np.inf, -np.inf], 0)
df_pamt = df_pamt[["PAY_AMT2","PAY_AMT3","PAY_AMT4","PAY_AMT5","PAY_AMT6"]].rename(columns={"PAY_AMT2":"PAY_AMTT1_2", "PAY_AMT3":"PAY_AMT_2_3", "PAY_AMT4":"PAY_AMT_3_4", "PAY_AMT5":"PAY_AMT4_5","PAY_AMT6":"PAY_AMT_5_6"})
df = pd.concat((df,df_pamt),axis=1)

df["PAY_AMT_1_6"]=df[["PAY_AMT1","PAY_AMT6"]].pct_change(axis='columns').replace([np.nan,np.inf, -np.inf], 0)["PAY_AMT6"]
df["BILL_AMT_1_6"]=df[["BILL_AMT1","BILL_AMT6"]].pct_change(axis='columns').replace([np.nan,np.inf, -np.inf], 0)["BILL_AMT6"]
df["PAY_0_6"]=df[["PAY_0","PAY_6"]].pct_change(axis='columns').replace([np.nan,np.inf, -np.inf], 0)["PAY_6"]
df.head()

plt.figure(figsize=(15,7))
sns.heatmap(df.corr(), cmap='seismic');

"""# Modelling"""

df.drop(columns=["ID"],inplace=True)

y = df["default payment next month"]
X = df.drop(columns=["default payment next month"])

X_train, X_test, y_train, y_test = mod.train_test_split(X, y, test_size=0.33, random_state=42,stratify=y)

def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,
                        n_jobs=1, train_sizes=np.linspace(0.01, 1.0, 20),scoring="f1"):
    plt.figure(figsize=(10,6))
    plt.title(title)
    if ylim is not None:
        plt.ylim(*ylim)
    plt.xlabel("Training examples")
    plt.ylabel("Score")
    train_sizes, train_scores, test_scores = learning_curve(
        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes,scoring=scoring)
    train_scores_mean = np.mean(train_scores, axis=1)
    train_scores_std = np.std(train_scores, axis=1)
    test_scores_mean = np.mean(test_scores, axis=1)
    test_scores_std = np.std(test_scores, axis=1)

    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,
                     train_scores_mean + train_scores_std, alpha=0.1,
                     color="r")
    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,
                     test_scores_mean + test_scores_std, alpha=0.1, color="g")
    plt.plot(train_sizes, train_scores_mean, '-', color="r",
             label="Training score")
    plt.plot(train_sizes, test_scores_mean, '-', color="g",
             label="Cross-validation score")

    plt.legend(loc="best")
    plt.grid("on")
    return plt

def ROC_plotter(y_train_,y_test_,X_train_,X_test_,estimator):
    plt.figure(figsize=(14,4))
    plt.subplot(121)
    fpr, tpr, tresholds = met.roc_curve(y_train_ , estimator.predict_proba(X_train_)[:,1])
    roc_auc = met.roc_auc_score(y_train_ , estimator.predict_proba(X_train_)[:,1])
    plt.plot(fpr, tpr, label='ROC-AUC = %0.2f' % roc_auc, color='darkorange', linestyle='dashdot', lw=2)
    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('ROC Curve - Train ')
    plt.legend(loc="lower right")
    plt.subplot(122)
    fpr, tpr, tresholds = met.roc_curve(y_test_ , estimator.predict_proba(X_test_)[:,1] )
    roc_auc = met.roc_auc_score(y_test_ , estimator.predict_proba(X_test_)[:,1])
    plt.plot(fpr, tpr, label='ROC-AUC = %0.2f' % roc_auc, color='darkorange', linestyle='dashdot', lw=2)
    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('ROC Curve - Test')
    plt.legend(loc="lower right")
    plt.show()

def PrecisionRecallCurve(y_train_,y_test_,X_train_,X_test_,estimator):
    plt.figure(figsize=(14,4))
    plt.subplot(121)
    plt.title("Precision vs Recall Training")
    precision, recall, tresholds = met.precision_recall_curve(y_train_,estimator.predict_proba(X_train_)[:,1])
    plt.plot(tresholds,precision[:-1],"--",color="navy",label="Precison: TP / (TP+FP)")
    plt.plot(tresholds,recall[:-1],"--",color="darkorange",label='Recall: TP / (TP+FN)')
    plt.legend()
    plt.subplot(122)
    plt.title("Precision vs Recall Test")
    precision, recall, tresholds = met.precision_recall_curve(y_test_,estimator.predict_proba(X_test_)[:,1])
    plt.plot(tresholds,precision[:-1],"--",color="navy",label="Precison: TP / (TP+FP)")
    plt.plot(tresholds,recall[:-1],"--",color="darkorange",label='Recall: TP / (TP+FN)')
    plt.legend()
    plt.show()
    
def PrecisionRecallCurve2(y_train_,y_test_,X_train_,X_test_,estimator):
    plt.figure(figsize=(14,4))
    plt.subplot(121)
    plt.title("Precision vs Recall Training")
    precision, recall, tresholds = met.precision_recall_curve(y_train_,estimator.predict_proba(X_train_)[:,1])
    pr_auc = met.auc(recall, precision)
    plt.plot(recall,precision,"--",color="darkorange",label='LR (PR-AUC = %0.2f)' % pr_auc)
    no_skill = len(y_train_[y_train_==1]) / len(y_train_)
    plt.plot([0, 1], [no_skill, no_skill], linestyle='--', color='navy', label='No Skill')
    plt.legend()
    plt.xlabel("precision")
    plt.ylabel("recall")
    plt.title("precision recall curve")
    plt.subplot(122)
    plt.title("Precision vs Recall Test")
    precision, recall, tresholds = met.precision_recall_curve(y_test_,estimator.predict_proba(X_test_)[:,1])
    pr_auc = met.auc(recall, precision)
    plt.plot(recall,precision,"--",color="darkorange",label='LR (PR-AUC = %0.2f)' % pr_auc)
    no_skill = len(y_test[y_test==1]) / len(y_test)
    plt.plot([0, 1], [no_skill, no_skill], linestyle='--', color='navy', label='No Skill')
    plt.legend()
    plt.xlabel("precision")
    plt.ylabel("recall")
    plt.show()

"""# RANDOM FOREST"""

import warnings
warnings.filterwarnings('ignore') 
pipeline = pip.Pipeline(steps=[ #('prep',preprocessor),
                                ('clf', ens.RandomForestClassifier(random_state=42))
                              ])
params = {
    'clf__criterion': ['entropy', 'gini'],
    'clf__max_features': [6,8,10,"auto"],
    'clf__max_depth': [ 8,10],
    'clf__min_samples_leaf': [1,2, 3],
    'clf__min_samples_split': [1,2, 3],
    'clf__class_weight':[ "balance_subsample","balanced",None]}

kfold = mod.StratifiedKFold(n_splits=3,random_state=42)
grid  = mod.GridSearchCV(estimator=pipeline, param_grid=params, cv=kfold, scoring="f1",verbose=1,n_jobs=-1)
grid.fit(X_train,y_train)
print(); print("Best CV score: %f using %s\n" % (grid.best_score_, grid.best_params_))
print("Training")
print(met.classification_report(y_train, grid.predict(X_train)))
print("Test")
print(met.classification_report(y_test, grid.predict(X_test)))
ROC_plotter(y_train,y_test,X_train,X_test,grid.best_estimator_)
PrecisionRecallCurve(y_train,y_test,X_train,X_test,grid.best_estimator_)
PrecisionRecallCurve2(y_train,y_test,X_train,X_test,grid.best_estimator_)
title = 'Learning Curves for RF'
kfold = mod.StratifiedKFold(n_splits=3, shuffle=True, random_state=42)
plot_learning_curve(grid.best_estimator_, title, X_train, y_train, cv=kfold)
plt.show()

importances = grid.best_estimator_.named_steps["clf"].feature_importances_
std = np.std([grid.best_estimator_.named_steps["clf"].feature_importances_ for tree in  grid.best_estimator_.named_steps["clf"].estimators_], axis=0)
indices = np.argsort(importances)[::-1]
feature_names = X_train.columns.tolist()
# Print the feature ranking
print("Feature ranking:")
feature_list = []
for f in range(X.shape[1]):
#    print("%d. feature %d (%f)" % (f + 1, indices[f], importances[indices[f]]))
    feature_list.append(feature_names[indices[f]])
    print("%2d. %15s %2d (%f)" % (f + 1, feature_names[indices[f]], indices[f], importances[indices[f]]))
# Plot the feature importances of the forest
plt.figure(figsize=(15,6))
plt.title("Feature importances")
plt.bar(range(X.shape[1]), importances[indices],
       color="r", yerr=std[indices], align="center")
#plt.xticks(range(X.shape[1]), indices, rotation='vertical')
plt.xticks(range(X.shape[1]), feature_list, rotation='vertical')
plt.xlim([-1, X.shape[1]])
plt.show()

"""The differences bw Training and Test performance in F1, PRAUC and ROCAUC metrics are high. Learning curve shows that model requires more complexity.
I tried a model with regularization but i did not get better results. I will keep them on the notebook.

# Using RF model with other sampling methods
"""

val = 0.3
import warnings
warnings.filterwarnings('ignore') 
for sampler in [ove.RandomOverSampler(sampling_strategy=val,random_state = 42),ove.SMOTE(sampling_strategy=val,random_state=42), und.RandomUnderSampler(sampling_strategy=val,random_state=42), und.TomekLinks(sampling_strategy="majority"),  com.SMOTEENN(random_state=42)]:
    pipeline = imbPipe.Pipeline([("sampling", sampler),
                                 ('clf', ens.RandomForestClassifier(random_state = 42))])
    params = {
    'clf__criterion': ['entropy'],
    'clf__max_features': [6,8],
    'clf__max_depth': [ 8,6],
    'clf__min_samples_leaf': [1],
    'clf__min_samples_split': [3],
    'clf__class_weight':[ "balance_subsample","balanced",None]}
    kfold = mod.StratifiedKFold(n_splits=3,random_state=42)
    grid  = mod.GridSearchCV(estimator=pipeline, param_grid=params, cv=kfold, scoring="f1",verbose=1,n_jobs=-1)
    grid.fit(X_train,y_train)
    print()
    print("Best CV score: %f using %s\n %s\n" % (grid.best_score_, grid.best_params_,sampler.__class__.__name__))
    pred_train = grid.predict(X_train)
    pred_test = grid.predict(X_test)
    pred_train_p = grid.predict_proba(X_train)
    pred_test_p = grid.predict_proba(X_test)
    print()
    print(sampler.__class__.__name__,"Training")
    print(met.classification_report(y_train, pred_train))
    print(sampler.__class__.__name__,"Test")
    print(met.classification_report(y_test, pred_test))
    print()

"""The best model I built is the random forest model with balanced class weights and SMOTE with 55 F1 score. I tried the sampling methods but they gave the similar results.

# XGBoost
"""

lrList = [0.01, 0.03, 0.05, 0.07, 0.1, 0.2]
plt.figure(figsize=(15,12))
j = 0
for lr in lrList:
    j += 1 
    trn_loss = [] ; val_loss = []
    for nest in np.linspace(50,400,50, dtype=int):
        clf = XGBClassifier(n_estimators=nest, learning_rate=lr, verbose=0,random_state=42,verbosity=0,use_label_encoder=False,n_jobs=-1)
        clf.fit(X_train, y_train)
        trn_loss.append(met.log_loss(y_train, clf.predict_proba(X_train)))
        val_loss.append(met.log_loss(y_test, clf.predict_proba(X_test)))
    plt.subplot(2,3,j)
    plt.plot(np.linspace(50,400,50), trn_loss, '-r', label='training_loss')
    plt.plot(np.linspace(50,400,50), val_loss, '-b', label='val_loss')
    plt.title("Learning rate = {0}".format(lr))
    plt.ylabel('Error')
    plt.xlabel('num_components')
    plt.legend(loc='upper right')
plt.show()

pipeline = pip.Pipeline(steps=[ #('prep',preprocessor),
                                ('clf', XGBClassifier(verbose=0,random_state=42,verbosity=0,use_label_encoder=False,n_jobs=-1))
                              ])
params = { 
              'clf__learning_rate'    : [0.05],
              'clf__n_estimators'     : [80],
              'clf__max_depth'        : [8,10],
              'clf__min_child_weight' : [50,60,70],
              'clf__gamma'            : [0.5,1,3],
              #'clf__subsample'       : [ 0.9, 1],
              #'clf__colsample_bytree': [ 0.9, 1],
              'clf__reg_alpha'        : [ 1, 3],
              'clf__reg_lambda'       : [ 2, 3]
              #'clf__class_pos_weigh' : [1, 10, 25, 50, 75, 99, 100, 1000,    
              #                         round(((y_train.value_counts().values / y_train.shape[0] ) * 100)[0]),
              #                         round(((y_train.value_counts().values / y_train.shape[0] ) * 100)[1])]
              #'clf__early_stopping_rounds' : [10]
            }

kfold = mod.StratifiedKFold(n_splits=3)
grid  = mod.GridSearchCV(estimator=pipeline, param_grid=params, cv=kfold, scoring="f1",verbose=1,n_jobs=-1)
grid.fit(X_train,y_train)
print(); print("Best CV score: %f using %s\n" % (grid.best_score_, grid.best_params_))
print("Training")
print(met.classification_report(y_train, grid.predict(X_train)))
print("Test")
print(met.classification_report(y_test, grid.predict(X_test)))
ROC_plotter(y_train,y_test,X_train,X_test,grid.best_estimator_)
PrecisionRecallCurve(y_train,y_test,X_train,X_test,grid.best_estimator_)
PrecisionRecallCurve2(y_train,y_test,X_train,X_test,grid.best_estimator_)
title = 'Learning Curves for XGB'
kfold = mod.StratifiedKFold(n_splits=3, shuffle=True, random_state=42)
plot_learning_curve(grid.best_estimator_, title, X_train, y_train, cv=kfold)
plt.show()

"""# Using XGB model with other sampling methods"""

X_train, X_test, y_train, y_test = mod.train_test_split(X, y, test_size=0.33, random_state=42,stratify=y)
val = 0.3
import warnings
warnings.filterwarnings('ignore') 
for sampler in [ove.RandomOverSampler(sampling_strategy=val,random_state = 42),ove.SMOTE(sampling_strategy=val,random_state=42), und.RandomUnderSampler(sampling_strategy=val,random_state=42), und.TomekLinks(sampling_strategy="majority"),  com.SMOTEENN(random_state=42)]:
    pipeline = imbPipe.Pipeline([("sampling", sampler),
                                 ('clf', XGBClassifier(verbose=0,random_state=42,verbosity=0,use_label_encoder=False,n_jobs=-1))])
    params = { 
              'clf__learning_rate'    : [0.05],
              'clf__n_estimators'     : [80],
              'clf__max_depth'        : [8,10],
              'clf__min_child_weight' : [50,60,70],
              'clf__gamma'            : [0.5,1,3],
              'clf__reg_alpha'        : [1],
              'clf__reg_lambda'       : [3]
            }
    kfold = mod.StratifiedKFold(n_splits=3)
    grid  = mod.GridSearchCV(estimator=pipeline, param_grid=params, cv=kfold, scoring="f1",verbose=1,n_jobs=-1)
    grid.fit(X_train.values,y_train.values)
    print()
    print("Best CV score: %f using %s\n %s\n" % (grid.best_score_, grid.best_params_,sampler.__class__.__name__))
    pred_train = grid.predict(X_train.values)
    pred_test = grid.predict(X_test.values)
    pred_train_p = grid.predict_proba(X_train.values)
    pred_test_p = grid.predict_proba(X_test.values)
    print()
    print(sampler.__class__.__name__,"Training")
    print(met.classification_report(y_train.values, pred_train))
    print(sampler.__class__.__name__,"Test")
    print(met.classification_report(y_test.values, pred_test))
    print()

pipeline = pip.Pipeline(steps=[ #('prep',preprocessor),
                                ('poly',pre.PolynomialFeatures(degree=2)),
                                ('clf', XGBClassifier(verbose=0,random_state=42,verbosity=1,use_label_encoder=False,n_jobs=-1))
                              ])
params = { 
              'clf__learning_rate'    : [0.05],
              'clf__n_estimators'     : [80],
              'clf__max_depth'        : [8,10],
              'clf__min_child_weight' : [50,60,70,100],
              'clf__gamma'            : [0.5,1,3,10],
              #'clf__subsample'       : [ 0.9, 1],
              #'clf__colsample_bytree': [ 0.9, 1],
              #'clf__reg_alpha'        : [ 1, 3],
              #'clf__reg_lambda'       : [ 2, 3]
              #'clf__class_pos_weigh' : [1, 10, 25, 50, 75, 99, 100, 1000,    
              #                         round(((y_train.value_counts().values / y_train.shape[0] ) * 100)[0]),
              #                         round(((y_train.value_counts().values / y_train.shape[0] ) * 100)[1])]
              #'clf__early_stopping_rounds' : [10]
            }

kfold = mod.StratifiedKFold(n_splits=3)
grid  = mod.GridSearchCV(estimator=pipeline, param_grid=params, cv=kfold, scoring="f1",verbose=1,n_jobs=-1)
grid.fit(X_train,y_train)
print(); print("Best CV score: %f using %s\n" % (grid.best_score_, grid.best_params_))
print("Training")
print(met.classification_report(y_train, grid.predict(X_train)))
print("Test")
print(met.classification_report(y_test, grid.predict(X_test)))
ROC_plotter(y_train,y_test,X_train,X_test,grid.best_estimator_)
PrecisionRecallCurve(y_train,y_test,X_train,X_test,grid.best_estimator_)
PrecisionRecallCurve2(y_train,y_test,X_train,X_test,grid.best_estimator_)
title = 'Learning Curves for XGB'
kfold = mod.StratifiedKFold(n_splits=3, shuffle=True, random_state=42)
plot_learning_curve(grid.best_estimator_, title, X_train, y_train, cv=kfold)
plt.show()